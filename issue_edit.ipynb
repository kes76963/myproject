{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "*핵심 키워드 어떤 방식으로 추출할 것인가? 이슈가 긍정인가 부정인가? \n",
    "\n",
    "공부해야할 부분\n",
    "\n",
    "-seq2seq + attention =>  문장 요약 & 생성\n",
    "-Text rank => sentence transformer 문장 유사도 사용해보기\n",
    "-문장 긍정 부정 (모델 있음)\n",
    "-이슈의 대상이 사람인지, 사건인지? (모델 없어보임)\n",
    "-이슈 카테고리 분류 cnn 모델로 하면 될 듯?\n",
    "\n",
    "=> 이후에 조회수, 키워드에 따른 알림 서비스 만들어야할 듯\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-28 22:25\n"
     ]
    }
   ],
   "source": [
    "#현재 시간 설정\n",
    "from datetime import datetime, date, time, timedelta\n",
    "\n",
    "now = datetime.now()\n",
    "nowDatetime = now.strftime('%Y-%m-%d %H:%M')\n",
    "print(nowDatetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['페이커', '버튜', '윤석열', '일본', '이재명', '한국', '취임식', '미드', '맨시티', '로아', '황희찬', '플립', '대통령', '도깨비', '코로나', '레전드', '쇼메', '유튜브', '인스타', '카사', '드라마', '라이즈', '중국', '유재석', '아르테타', '아프간', '햄튼', '울버', '고스트']\n"
     ]
    }
   ],
   "source": [
    "########### 이슈 빼오기 ########\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.issuelink.co.kr/community/listview/all/3/adj/_self/blank/blank/blank'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36'}\n",
    "html_rank = requests.get(url, headers=headers).text\n",
    "soup_rank = BeautifulSoup(html_rank, 'html.parser')\n",
    "# keyword = soup_rank.select('div > div:nth-of-type(2) > div:nth-of-type(4) > div.ibox.float-e-margins > div ')\n",
    "keywords = soup_rank.select('div.ibox.float-e-margins > div > table > tbody > tr > td > a')\n",
    "\n",
    "\n",
    "key_list = []\n",
    "for k in keywords:\n",
    "    keyword = k.text\n",
    "    key_list.append(keyword)\n",
    "\n",
    "###### 7시를 기준으로 기준 리스트를 하나 만들어야 함 / datetime에서 시간 분만 빼와서 if로 비교\n",
    "##### 7시 기준이 아닐 때 \n",
    "\n",
    "key_list.pop(0)\n",
    "print(key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************\n",
      "{'호날두': 135071, '일본': 126069, '아프간': 124348, '한국': 128719, '펜트하우스': 12452, '이재명': 25445, '윤희숙': 31888, '법무부': 36026, '플립': 22204, '김용호': 32121, '도깨비': 54311, '블리치': 31913, '로아': 2143, '윤석열': 8096, '홍준표': 6027, '보라': 5639, '이야기': 10234, '코로나': 31626, '미국': 24386, '레전드': 43250, '아이돌': 32196, '이낙연': 12644, '아파트': 29632, '민주당': 6260, '유튜브': 14252, '중국': 78382, '갤럭시': 20462, '정윤종': 2798, '마지막': 15996}\n"
     ]
    }
   ],
   "source": [
    "### 이슈 실제 조회수 ###\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import operator\n",
    "\n",
    "sum_list = []\n",
    "search_list = key_list\n",
    "\n",
    "def sum_hit(search) :\n",
    "    \n",
    "    url = f'https://www.issuelink.co.kr/community/listview/read/3/adj/_self/blank/{search}'\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36'}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    hits = soup.select('span.hit')\n",
    "    \n",
    "    sum=0\n",
    "    \n",
    "    for hit in hits :\n",
    "        sum += int(hit.text.replace(',',''))\n",
    "    \n",
    "    sum_list.append(sum)  \n",
    "    print('*',end='')\n",
    "    \n",
    "for search in search_list :\n",
    "    sum_hit(search)\n",
    "\n",
    "#조회수, 키워드 합치기\n",
    "sum_search = dict(zip(key_list,sum_list))\n",
    "\n",
    "#조회수 순으로 정렬\n",
    "#a = sorted(sum_search.items(), key=lambda x:x[1], reverse = True)\n",
    "\n",
    "print()\n",
    "print(sum_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************\n",
      "완료\n"
     ]
    }
   ],
   "source": [
    "#### 키워드 제목 추출 탐30 개별로 ####\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "#디렉토리 폴더 생성\n",
    "path = \"C:/work/python/Asia_GAN/myproject/temp\" #C:\\work\\python\\Asia_GAN\\myproject\n",
    "if not os.path.isdir(path):                                                           \n",
    "    os.mkdir(path)\n",
    "\n",
    "keyword_list=[]\n",
    "\n",
    "def subject(search) :\n",
    "    url = f'https://www.issuelink.co.kr/community/listview/all/3/adj/_self/blank/{search}'\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36'}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    sub = soup.select('span.title')\n",
    "    \n",
    "    keyword_list.clear()\n",
    "    \n",
    "    for i in sub :\n",
    "        split_string = i.get_text().split(' [',1)\n",
    "        substring = split_string[0]    \n",
    "        keyword_list.append(substring)\n",
    "        \n",
    "    with open(f'C:/work/python/Asia_GAN/myproject/temp/{search}.txt','w', encoding = 'utf-8') as file :\n",
    "        file.writelines(keyword_list)\n",
    "    \n",
    "    print('**', end=\"\")\n",
    "            \n",
    "    \n",
    "for search in search_list :\n",
    "    subject(search)\n",
    "\n",
    "print()\n",
    "print('완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kim\\.conda\\envs\\exam_cv2\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료\n"
     ]
    }
   ],
   "source": [
    "#### 키워드 형태소 카운팅 ####\n",
    "\n",
    "\"\"\" 형태소 분석기\n",
    "    명사 추출 및 빈도수 체크\n",
    "    python [모듈 이름] [텍스트 파일명.txt] [결과파일명.txt]\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from konlpy.tag import Twitter\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_tags(text, ntags=50):\n",
    "    spliter = Twitter()\n",
    "    nouns = spliter.nouns(text)\n",
    "    count = Counter(nouns)\n",
    "    return_list = []\n",
    "    for n, c in count.most_common(ntags):\n",
    "        temp = {'tag': n, 'count': c}\n",
    "        return_list.append(temp)\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def main(search):\n",
    "    # 분석할 파일\n",
    "    noun_count = 50\n",
    "    # count.txt 에 저장\n",
    "    open_text_file = open(f'C:/work/python/Asia_GAN/myproject/temp/{search}.txt', 'r',-1,\"utf-8\")\n",
    "    # 분석할 파일을 open \n",
    "    text = open_text_file.read() #파일을 읽습니다.\n",
    "    tags = get_tags(text, noun_count) # get_tags 함수 실행\n",
    "    open_text_file.close()   #파일 close\n",
    "    open_output_file = open(f\"C:/work/python/Asia_GAN/myproject/temp/{search}-count.txt\", 'w',-1,\"utf-8\")\n",
    "    # 결과로 쓰일 count.txt 열기\n",
    "    for tag in tags:\n",
    "        noun = tag['tag']\n",
    "        count = tag['count']\n",
    "        open_output_file.write('{} {}\\n'.format(noun, count))\n",
    "    # 결과 저장\n",
    "    open_output_file.close() \n",
    "\n",
    "for search in search_list :\n",
    "    main(search)\n",
    "    \n",
    "print('완료')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'호날두': '맨유', '일본': '아프간', '아프간': '작전', '한국': '아프간', '펜트하우스': '단태', '이재명': '사생활', '윤희숙': '숙', '법무부': '차관', '플립': '갤럭시', '김용호': '방송', '도깨비': '게임', '블리치': '만해', '로아': '좀', '윤석열': '이재명', '홍준표': '이유', '보라': '김보라', '이야기': '지금', '코로나': '검사', '미국': '아프간', '레전드': '기사', '아이돌': '그룹', '이낙연': '이재명', '아파트': '공급', '민주당': '지지자', '유튜브': '김용호', '중국': '보고', '갤럭시': '폴드', '정윤종': '김윤환', '마지막': '김용호'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "sub_key = {}\n",
    "\n",
    "for i in sum_search :\n",
    "      \n",
    "    with open(f'C:/work/python/Asia_GAN/myproject/temp/{i}-count.txt','r', encoding = 'utf-8') as file :\n",
    "        data = str(file.readlines()[1])\n",
    "\n",
    "    split_string = data.split(' ',1) \n",
    "    substring = split_string[0]           #빈도수 제거 \n",
    "    #print(substring)\n",
    "    \n",
    "    sub_key[i] = substring\n",
    "    \n",
    "    \n",
    "print(sub_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['호날두', 'https://sports.news.naver.com/news.nhn?oid=076&aid=0003771921', 'https://sports.news.naver.com/news.nhn?oid=108&aid=0002983994'], ['한국', 'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=104&oid=001&aid=0012625367', 'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=100&oid=366&aid=0000757380'], ['일본', 'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=104&oid=003&aid=0010687901', 'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=104&oid=417&aid=0000729372'], ['아프간', 'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=104&oid=022&aid=0003614026', 'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=100&oid=018&aid=0005021393'], ['중국', 'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=104&oid=003&aid=0010685077', 'https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=104&oid=056&aid=0011108320']]\n",
      "dict_items([('호날두', 135071), ('일본', 126069), ('아프간', 124348), ('한국', 128719), ('펜트하우스', 12452), ('이재명', 25445), ('윤희숙', 31888), ('법무부', 36026), ('플립', 22204), ('김용호', 32121), ('도깨비', 54311), ('블리치', 31913), ('로아', 2143), ('윤석열', 8096), ('홍준표', 6027), ('보라', 5639), ('이야기', 10234), ('코로나', 31626), ('미국', 24386), ('레전드', 43250), ('아이돌', 32196), ('이낙연', 12644), ('아파트', 29632), ('민주당', 6260), ('유튜브', 14252), ('중국', 78382), ('갤럭시', 20462), ('정윤종', 2798), ('마지막', 15996)])\n"
     ]
    }
   ],
   "source": [
    "#네이버 검색, 키워드 서브키워드 \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "art_lists = []\n",
    "\n",
    "def search(key, b) :\n",
    "    \n",
    "    art_list = [b]\n",
    "    \n",
    "    #url = f'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query={key}'\n",
    "    url = f'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={key}'\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    for i in range(2) :\n",
    "        news = soup.select('div.info_group > a:nth-of-type(2)')[i].attrs[\"href\"]\n",
    "        art_list.append(news)\n",
    "    \n",
    "    art_lists.append(art_list)\n",
    "    \n",
    "a = sorted(sum_search.items(), key=lambda x:x[1], reverse = True) #value 값 기준으로 정렬, 상위 5개 키워드\n",
    "\n",
    "\n",
    "for i in range(5) :\n",
    "    b= a[i][0]     #정렬 후 dic -> list 함수로 변환돼서 [i][0]으로 빼옴 \n",
    "    #print(b)\n",
    "    \n",
    "    with open(f'C:/work/python/Asia_GAN/myproject/temp/{b}-count.txt','r', encoding = 'utf-8') as file :\n",
    "        data = str(file.readlines()[1])\n",
    "\n",
    "    split_string = data.split(' ',1) \n",
    "    substring = split_string[0]           #빈도수 제거 \n",
    "    #print(substring)\n",
    "    \n",
    "    key = b + \" \" + substring\n",
    "    search(key, b)\n",
    "    \n",
    "print(art_lists)\n",
    "print(sum_search.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory is deleted successfully\n"
     ]
    }
   ],
   "source": [
    "#subject 디렉토리내 파일 삭제하기\n",
    "\n",
    "import shutil\n",
    "\n",
    "pathTest = r\"C:/work/python/Asia_GAN/myproject/temp\"\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(pathTest)\n",
    "except OSError as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"The directory is deleted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일을 작업중입니다....\n"
     ]
    }
   ],
   "source": [
    "#### 새로운 파일 생성 ###\n",
    "\n",
    "import os\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import datetime\n",
    "import openpyxl\n",
    "import pickle\n",
    "\n",
    "#디렉토리 폴더 생성\n",
    "path = \"C:/work/python/Asia_GAN/myproject/output\"\n",
    "if not os.path.isdir(path):                                                           \n",
    "    os.mkdir(path)\n",
    "\n",
    "\n",
    "today =datetime.datetime.now()\n",
    "\n",
    "hms = today.strftime('%H:%M:%S') #시분초\n",
    "ymd = today.strftime('%Y-%m-%d') #년월일  - 오늘 날짜\n",
    "yesterday = (today - datetime.timedelta(1)).strftime('%Y-%m-%d') #어제날짜\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "\n",
    "\n",
    "if hms < '19:00:00':\n",
    "    file_name = yesterday\n",
    "    path2 = f\"C:/work/python/Asia_GAN/myproject/output/{file_name}.xlsx\"\n",
    "    \n",
    "    if not os.path.isfile(path2):  \n",
    "        wb.save(path2)\n",
    "        print('파일 생성완료1')\n",
    "\n",
    "    else :\n",
    "        print('파일을 작업중입니다....')\n",
    "\n",
    "else : \n",
    "    file_name = ymd\n",
    "    path2 = f\"C:/work/python/Asia_GAN/myproject/output/{file_name}.xlsx\"\n",
    "    \n",
    "    if not os.path.isfile(path2):  \n",
    "        wb.save(path2)\n",
    "        print('파일 생성완료2')\n",
    "\n",
    "    else :\n",
    "        print('파일을 작업중입니다....')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### 행 생성 ####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "# import xlsxwriter\n",
    "\n",
    "#복제\n",
    "key_lists = list(chain.from_iterable(zip(key_list,key_list))) #멀티 인덱스를 위해서 [a,b,c] => [a,a,b,b,c,c] 로 만들어줌\n",
    "multi_keys = ['조회수','서브키워드'] * len(key_list)\n",
    "\n",
    "# 조회수, 서브 키워드 입력\n",
    "def hit_sub(i, time) :\n",
    "    df1.loc[time][(i,'조회수')] = sum_search[i]\n",
    "    df1.loc[time][(i,'서브키워드')] = sub_key[i]\n",
    "    \n",
    "# generate time series index\n",
    "time = nowDatetime    #현재 시간 설정\n",
    "\n",
    "df1 = pd.DataFrame(columns = [key_lists, multi_keys],index = [time])\n",
    "\n",
    "for i in sum_search : \n",
    "    hit_sub(i, time)\n",
    "    \n",
    "#### 다음 행에 추가 ####\n",
    "df = pd.read_excel(path2, header=[0,1], index_col=[0] )   #number 해결\n",
    "df = df.append(df1, sort=False)    #컬럼 수가 다를 때 NaN 값을 넣고 행 추가\n",
    "\n",
    "# ##### 열 너비 조정 ####\n",
    "# writer = pd.ExcelWriter(path2, engine='xlsxwriter')\n",
    "# df.to_excel(writer, sheet_name=\"Sheet1\")\n",
    "# workbook = writer.book\n",
    "# worksheet = writer.sheets[\"Sheet1\"]\n",
    "# worksheet.set_column('A:A', 20)\n",
    "# writer.save()\n",
    "# print('행 추가 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'levels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2aa3fda83958>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mhit_top5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Index' object has no attribute 'levels'"
     ]
    }
   ],
   "source": [
    "#### 상위 5개 키워드 ####\n",
    "\n",
    "df = pd.read_excel(path2, header=[0,1], index_col=[0] )\n",
    "df_col = list(df.columns.levels[0])\n",
    "\n",
    "hit_top5 = []\n",
    "\n",
    "for i in df_col :\n",
    "    hit_max = df[i,'조회수'].max()\n",
    "    hit_top5.append(hit_max)\n",
    "\n",
    "hit_dict = dict(zip(df_col,hit_top5)) #딕셔너리 값으로 저장\n",
    "\n",
    "keyword_top5 = sorted(hit_dict, key=hit_dict.get, reverse = True)[:5] #탑 5개 keyword 추출\n",
    "print(keyword_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그래프 시각화 완료\n"
     ]
    }
   ],
   "source": [
    "#### 19시에 코드 보내기\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "title_font = {'fontsize': 16, 'fontweight': 'bold'}\n",
    "plt.rc('font',family='Malgun Gothic')\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.style.use('ggplot')\n",
    "plt.show(block=False)\n",
    "plt.pause(1)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "출처: https://theonly1.tistory.com/2470 [Be the only one, not the best one]\n",
    "\n",
    "ymd = today.strftime('%Y-%m-%d') \n",
    "hms = '19:00:00'\n",
    "\n",
    "if hms == '19:00:00':\n",
    "    path_png = \"C:/Workspace/project2_final/output/graphs\"\n",
    "    if not os.path.isdir(path_png):                                                           \n",
    "        os.mkdir(path_png)\n",
    "    \n",
    "    path_date = f\"C:/Workspace/project2_final/output/graphs/{ymd}\"\n",
    "    if not os.path.isdir(path_date):                                                           \n",
    "        os.mkdir(path_date)    \n",
    "    \n",
    "    ##### 그래프 조회수 변동 ######\n",
    "    for i in keyword_top5 :\n",
    "        plt.title (f\"키워드 : '{i}' 조회수 변동\", fontsize=20)\n",
    "        df.index,df[i,'조회수'].plot( kind='bar')\n",
    "        #    plt.bar(df.index,df[i,'조회수'])\n",
    "        #plt.show()\n",
    "        plt.savefig(f'{path_date}/조회수변동-({i}).png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "                 #.plot( kind='bar')\n",
    "\n",
    "    ##### 그래프 키워드 관심도 #####\n",
    "    for i in keyword_top5 :\n",
    "        text =f'{i} 키워드 관심도'\n",
    "        sub_keys = df[i].groupby(['서브키워드'])['조회수'].mean().sort_values()\n",
    "        plt.title(text, fontdict=title_font, loc='center', pad= 20)\n",
    "        sub_keys.plot(kind='pie', autopct = '%1.1f%%', shadow = True, startangle=110 )\n",
    "        #plt.show()\n",
    "\n",
    "        plt.savefig(f'{path_date}/관심도-({i}).png')\n",
    "        plt.close()\n",
    "    \n",
    "    print('그래프 시각화 완료')\n",
    "    \n",
    " \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19a2027c367e4a8fbf50703f7b521df71edff403eb9eba2200ef5f1febf03a5b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('exam_cv2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
