{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from konlpy.tag import Twitter\r\n",
    "import nltk\r\n",
    "\r\n",
    "twitter = Twitter()\r\n",
    "\r\n",
    "print(twitter.morphs(u'한글형태소분석기 테스트 중 입니다')) # ??\r\n",
    "print(twitter.nouns(u'한글형태소분석기 테스트 중 입니다!')) #명사\r\n",
    "print(twitter.pos(u'한글형태소분석기 테스트 중 입니다.')) #형태소"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kim\\.conda\\envs\\exam_cv2\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['한글', '형태소', '분', '석기', '테스트', '중', '입니다']\n",
      "['한글', '형태소', '석기', '테스트', '중']\n",
      "[('한글', 'Noun'), ('형태소', 'Noun'), ('분', 'Suffix'), ('석기', 'Noun'), ('테스트', 'Noun'), ('중', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def read_data(filename):\r\n",
    "    with open(filename, 'r', encoding = \"utf-8\") as f:\r\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\r\n",
    "    return data\r\n",
    "\r\n",
    "def tokenize(doc):\r\n",
    "  # norm, stem은 optional\r\n",
    "  return ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]\r\n",
    "\r\n",
    "def term_exists(doc):\r\n",
    "    return {'exists({})'.format(word): (word in set(doc)) for word in selected_words}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# 트래이닝 데이터와 테스트 데이터를 읽기\r\n",
    "train_data = read_data('./data/ratings_train.txt')\r\n",
    "test_data = read_data('./data/ratings_test.txt')\r\n",
    "\r\n",
    "# row, column의 수가 제대로 읽혔는지 확인\r\n",
    "print(len(train_data))      # nrows: 150000\r\n",
    "print(len(train_data[0]))   # ncols: 3\r\n",
    "print(len(test_data))       # nrows: 50000\r\n",
    "print(len(test_data[0]))     # ncols: 3\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "150001\n",
      "3\n",
      "50001\n",
      "3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 형태소 분류\r\n",
    "train_docs = [(tokenize(row[1]), row[2]) for row in train_data[1:]]\r\n",
    "test_docs = [(tokenize(row[1]), row[2]) for row in test_data[1:]]\r\n",
    "\r\n",
    "#Training data의 token 모으기\r\n",
    "tokens = [t for d in train_docs for t in d[0]]\r\n",
    "print(len(tokens))\r\n",
    "\r\n",
    "# Load tokens with nltk.Text()\r\n",
    "text = nltk.Text(tokens, name='NMSC')\r\n",
    "print(text.vocab().most_common(10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2159921\n",
      "[('./Punctuation', 67778), ('영화/Noun', 50818), ('하다/Verb', 41209), ('이/Josa', 38540), ('보다/Verb', 38538), ('의/Josa', 30188), ('../Punctuation', 29055), ('가/Josa', 26627), ('에/Josa', 26468), ('을/Josa', 23118)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# 텍스트간의 연어 빈번하게 등장하는 단어 구하기\r\n",
    "text.collocations()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "이/Determiner 것/Noun; 적/Suffix 인/Josa; 이/Determiner 거/Noun; 것/Noun\n",
      "은/Josa; 10/Number 점/Noun; 배우/Noun 들/Suffix; 이/Noun 게/Josa; 수/Noun\n",
      "있다/Adjective; 내/Noun 가/Josa; 최고/Noun 의/Josa; 네/Suffix 요/Josa; 이/Noun\n",
      "영화/Noun; 들/Suffix 이/Josa; 끝/Noun 까지/Josa; 때문/Noun 에/Josa; 적/Suffix\n",
      "으로/Josa; 못/VerbPrefix 하다/Verb; 사람/Noun 들/Suffix; 1/Number 점/Noun;\n",
      "영화/Noun 를/Josa\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# term이 존재하는지에 따라서 문서를 분류\r\n",
    "selected_words = [f[0] for f in text.vocab().most_common(2000)] # 여기서는 최빈도 단어 2000개를 피쳐로 사용\r\n",
    "train_docs = train_docs[:10000] # 시간 단축을 위한 꼼수로 training corpus의 일부만 사용할 수 있음\r\n",
    "train_xy = [(term_exists(d), c) for d, c in train_docs]\r\n",
    "test_xy = [(term_exists(d), c) for d, c in test_docs]\r\n",
    "\r\n",
    "# nltk의 NaiveBayesClassifier으로 데이터를 트래이닝 시키고, test 데이터로 확인\r\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_xy) #Naive Bayes classifier 적용\r\n",
    "print(nltk.classify.accuracy(classifier, test_xy))\r\n",
    "# => 0.80418\r\n",
    "\r\n",
    "classifier.show_most_informative_features(10)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.80414\n",
      "Most Informative Features\n",
      "         exists(수작/Noun) = True                1 : 0      =     38.0 : 1.0\n",
      "     exists(이딴/Modifier) = True                0 : 1      =     32.1 : 1.0\n",
      "         exists(최악/Noun) = True                0 : 1      =     30.1 : 1.0\n",
      "       exists(♥/Foreign) = True                1 : 0      =     24.5 : 1.0\n",
      "         exists(노잼/Noun) = True                0 : 1      =     22.1 : 1.0\n",
      "         exists(짜증/Noun) = True                0 : 1      =     19.5 : 1.0\n",
      "        exists(쓰레기/Noun) = True                0 : 1      =     19.4 : 1.0\n",
      "         exists(여운/Noun) = True                1 : 0      =     18.9 : 1.0\n",
      "          exists(굿/Noun) = True                1 : 0      =     17.1 : 1.0\n",
      "        exists(발연기/Noun) = True                0 : 1      =     16.9 : 1.0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('exam_cv2': conda)"
  },
  "interpreter": {
   "hash": "19a2027c367e4a8fbf50703f7b521df71edff403eb9eba2200ef5f1febf03a5b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}